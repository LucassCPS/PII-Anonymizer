# PII-Anonymizer

Este projeto implementa um conjunto de testes para modelos de linguagem em ambiente local utilizando o **Docker Desktop** com o **Model Runner**.  A aplicaÃ§Ã£o foi desenvolvida em Python e utiliza os modelos hospedados no **Model Hub do Docker**.

---

## Requisitos

Antes de iniciar, verifique se vocÃª possui os seguintes componentes instalados:
- [Docker Desktop](https://www.docker.com/products/docker-desktop/)
- [Docker Model Runner](https://docs.docker.com/ai/model-runner/)

---

## InstalaÃ§Ã£o

### 1. Clonar o repositÃ³rio

```bash
git clone https://github.com/LucassCPS/PII-Anonymizer.git
cd PII-Anonymizer
```
---

## ConfiguraÃ§Ã£o do Model Runner

Certifique-se de que o **Docker Desktop** estÃ¡ em execuÃ§Ã£o e que o **Model Runner** estÃ¡ instalado. Siga as instruÃ§Ãµes oficiais:  
[DocumentaÃ§Ã£o do Model Runner (Docker)](https://docs.docker.com/ai/model-runner/)

---

## Modelos NecessÃ¡rios

Baixe e registre os modelos listados abaixo diretamente do **Model Hub**:

| VariÃ¡vel de ambiente    | Nome do modelo                           | Link para o Docker Hub                                                                             |
| ----------------------- | ---------------------------------------- | -------------------------------------------------------------------------------------------------- |
| `MODEL_GEMMA3_QAT_4B`   | `ai/gemma3-qat:4B-UD-Q4_K_XL`            | [ðŸ”— Docker Hub - Gemma3 QAT 4B](https://hub.docker.com/r/ai/gemma3-qat)                            |
| `MODEL_GEMMA3_QAT_270M` | `ai/gemma3-qat:270M-UD-Q4_K_XL`          | [ðŸ”— Docker Hub - Gemma3 QAT 270M](https://hub.docker.com/r/ai/gemma3-qat)                          |
| `MODEL_SMOLL3_Q4`       | `ai/smollm3:Q4_K_M`                      | [ðŸ”— Docker Hub - SmolLM3 Q4](https://hub.docker.com/r/ai/smollm3)                                  |
| `MODEL_SMOLL3_Q8`       | `ai/smollm3:Q8_0`                        | [ðŸ”— Docker Hub - SmolLM3 Q8](https://hub.docker.com/r/ai/smollm3)                                  |
| `MODEL_QWEN3_06B`       | `ai/qwen3:0.6B-Q4_K_M`                   | [ðŸ”— Docker Hub - Qwen3 0.6B](https://hub.docker.com/r/ai/qwen3)                                    |
| `MODEL_QWEN3_8B`        | `ai/qwen3:8B-Q4_K_M`                     | [ðŸ”— Docker Hub - Qwen3 8B](https://hub.docker.com/r/ai/qwen3)                                      |
| `MODEL_GRANITE4_TINY`   | `ai/granite-4.0-h-tiny:7B`               | [ðŸ”— Docker Hub - Granite 4.0 Tiny](https://hub.docker.com/r/ai/granite-4.0-h-tiny)                 |
| `MODEL_MISTRAL_NEMO`    | `ai/mistral-nemo:12B-Q4_K_M`             | [ðŸ”— Docker Hub - Mistral Nemo 12B](https://hub.docker.com/r/ai/mistral-nemo)                       |
| `MODEL_DEEPSEEK`        | `ai/deepseek-r1-distill-llama:8B-Q4_K_M` | [ðŸ”— Docker Hub - DeepSeek R1 Distill LLaMA](https://hub.docker.com/r/ai/deepseek-r1-distill-llama) |
| `MODEL_LLAMA32_3B_Q4`   | `ai/llama3.2:3B-Q4_K_M`                  | [ðŸ”— Docker Hub - LLaMA 3.2 3B Q4](https://hub.docker.com/r/ai/llama3.2)                            |
| `MODEL_LLAMA32_1B_Q8`   | `ai/llama3.2:1B-Q8_0`                    | [ðŸ”— Docker Hub - LLaMA 3.2 1B Q8](https://hub.docker.com/r/ai/llama3.2)                            |

Baixe cada modelo executando o comando abaixo (substitua pelo nome do modelo):

```bash
docker pull ai/gemma3-qat:270M-UD-Q4_K_XL
```

---

## ExecuÃ§Ã£o

ApÃ³s configurar o ambiente, inicie o **Model Runner** com os modelos carregados e execute a aplicaÃ§Ã£o atravÃ©s de:

```bash
docker compose build && docker compose up -d && docker compose run --rm -T app python -u main.py
```

Desligue o ambiente atravÃ©s do comando:
```bash
docker compose down
```


**Obs. 1: Certifique-se de que o Model Runner estÃ¡ rodando no Docker Desktop. A aplicaÃ§Ã£o consumirÃ¡ os modelos atravÃ©s das chamadas configuradas via API local.**

**Obs. 2: Altere os parÃ¢metros de execuÃ§Ã£o diretamente no main.py (modelo, prompt, temperatura, numero de linhas do dataset e etc.)**
